{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-11-15T08:24:03.991053Z","iopub.execute_input":"2024-11-15T08:24:03.992142Z","iopub.status.idle":"2024-11-15T08:24:05.309449Z","shell.execute_reply.started":"2024-11-15T08:24:03.992085Z","shell.execute_reply":"2024-11-15T08:24:05.308245Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"%pip install -U -q \"google-generativeai>=0.8.3\"","metadata":{"execution":{"iopub.status.busy":"2024-11-15T08:24:06.989472Z","iopub.execute_input":"2024-11-15T08:24:06.990070Z","iopub.status.idle":"2024-11-15T08:24:37.781788Z","shell.execute_reply.started":"2024-11-15T08:24:06.990024Z","shell.execute_reply":"2024-11-15T08:24:37.780400Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Note: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"import google.generativeai as genai\nfrom IPython.display import HTML, Markdown, display","metadata":{"execution":{"iopub.status.busy":"2024-11-15T08:25:08.248401Z","iopub.execute_input":"2024-11-15T08:25:08.248946Z","iopub.status.idle":"2024-11-15T08:25:09.527470Z","shell.execute_reply.started":"2024-11-15T08:25:08.248897Z","shell.execute_reply":"2024-11-15T08:25:09.526243Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\n\nGOOGLE_API_KEY = UserSecretsClient().get_secret(\"GOOGLE_API_KEY\")\ngenai.configure(api_key=GOOGLE_API_KEY)","metadata":{"execution":{"iopub.status.busy":"2024-11-15T08:25:27.999561Z","iopub.execute_input":"2024-11-15T08:25:28.001417Z","iopub.status.idle":"2024-11-15T08:25:28.216246Z","shell.execute_reply.started":"2024-11-15T08:25:28.001345Z","shell.execute_reply":"2024-11-15T08:25:28.214929Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"flash = genai.GenerativeModel('gemini-1.5-flash')\nresponse = flash.generate_content(\"Explain Generative AI to me like I'm a kid.\")\nprint(response.text)","metadata":{"execution":{"iopub.status.busy":"2024-11-15T08:26:34.460330Z","iopub.execute_input":"2024-11-15T08:26:34.461514Z","iopub.status.idle":"2024-11-15T08:26:36.583141Z","shell.execute_reply.started":"2024-11-15T08:26:34.461460Z","shell.execute_reply":"2024-11-15T08:26:36.581630Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Imagine you have a magical box of LEGOs, but instead of just regular bricks, it has *all* the LEGOs ever made, and even some you've never seen before!  \n\nGenerative AI is like a super-smart kid who plays with this magical LEGO box.  You give them a picture of a castle, and they use the LEGOs to build one, even if they've never seen that *exact* castle before. They can even build you a spaceship or a dinosaur, all from the same box of LEGOs!\n\nIt learns from all the LEGOs it sees (that's all the information it's been shown) and then uses that knowledge to create something *new* and *original*.  It might not be perfect, but it's creative and can make amazing things!\n\nSo, Generative AI is a type of computer program that can create new things like pictures, stories, music, or even computer code, all by learning from the information it's been given. It's like a really imaginative LEGO master!\n\n","output_type":"stream"}]},{"cell_type":"code","source":"Markdown(response.text)","metadata":{"execution":{"iopub.status.busy":"2024-11-15T08:27:01.821595Z","iopub.execute_input":"2024-11-15T08:27:01.822063Z","iopub.status.idle":"2024-11-15T08:27:01.830944Z","shell.execute_reply.started":"2024-11-15T08:27:01.822019Z","shell.execute_reply":"2024-11-15T08:27:01.829773Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"Imagine you have a magical box of LEGOs, but instead of just regular bricks, it has *all* the LEGOs ever made, and even some you've never seen before!  \n\nGenerative AI is like a super-smart kid who plays with this magical LEGO box.  You give them a picture of a castle, and they use the LEGOs to build one, even if they've never seen that *exact* castle before. They can even build you a spaceship or a dinosaur, all from the same box of LEGOs!\n\nIt learns from all the LEGOs it sees (that's all the information it's been shown) and then uses that knowledge to create something *new* and *original*.  It might not be perfect, but it's creative and can make amazing things!\n\nSo, Generative AI is a type of computer program that can create new things like pictures, stories, music, or even computer code, all by learning from the information it's been given. It's like a really imaginative LEGO master!\n"},"metadata":{}}]},{"cell_type":"code","source":"chat = flash.start_chat(history=[])\nresponse = chat.send_message('Hello! My name is Zlork.')\nprint(response.text)","metadata":{"execution":{"iopub.status.busy":"2024-11-15T08:27:28.555761Z","iopub.execute_input":"2024-11-15T08:27:28.556184Z","iopub.status.idle":"2024-11-15T08:27:29.179240Z","shell.execute_reply.started":"2024-11-15T08:27:28.556137Z","shell.execute_reply":"2024-11-15T08:27:29.178110Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"It's nice to meet you, Zlork!  How can I help you today?\n\n","output_type":"stream"}]},{"cell_type":"code","source":"response = chat.send_message('Can you tell something interesting about dinosaurs?')\nprint(response.text)","metadata":{"execution":{"iopub.status.busy":"2024-11-15T08:27:46.748664Z","iopub.execute_input":"2024-11-15T08:27:46.749095Z","iopub.status.idle":"2024-11-15T08:27:47.687837Z","shell.execute_reply.started":"2024-11-15T08:27:46.749050Z","shell.execute_reply":"2024-11-15T08:27:47.686794Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"Many dinosaurs likely had feathers!  While we often picture dinosaurs as scaly reptiles,  evidence suggests that many, especially theropods (the group that includes *Tyrannosaurus rex* and *Velociraptor*), possessed feathers, ranging from simple filaments to complex, bird-like plumage.  This discovery significantly changed our understanding of dinosaur evolution and their relationship to birds.\n\n","output_type":"stream"}]},{"cell_type":"code","source":"# While you have the `chat` object around, the conversation state\n# persists. Confirm that by asking if it knows my name.\nresponse = chat.send_message('Do you remember what my name is?')\nprint(response.text)","metadata":{"execution":{"iopub.status.busy":"2024-11-15T08:28:00.145431Z","iopub.execute_input":"2024-11-15T08:28:00.145880Z","iopub.status.idle":"2024-11-15T08:28:00.659377Z","shell.execute_reply.started":"2024-11-15T08:28:00.145839Z","shell.execute_reply":"2024-11-15T08:28:00.658144Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"Yes, your name is Zlork.\n\n","output_type":"stream"}]},{"cell_type":"code","source":"for model in genai.list_models():\n  print(model.name)","metadata":{"execution":{"iopub.status.busy":"2024-11-15T08:28:14.768543Z","iopub.execute_input":"2024-11-15T08:28:14.768945Z","iopub.status.idle":"2024-11-15T08:28:14.957677Z","shell.execute_reply.started":"2024-11-15T08:28:14.768906Z","shell.execute_reply":"2024-11-15T08:28:14.956445Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"models/chat-bison-001\nmodels/text-bison-001\nmodels/embedding-gecko-001\nmodels/gemini-1.0-pro-latest\nmodels/gemini-1.0-pro\nmodels/gemini-pro\nmodels/gemini-1.0-pro-001\nmodels/gemini-1.0-pro-vision-latest\nmodels/gemini-pro-vision\nmodels/gemini-1.5-pro-latest\nmodels/gemini-1.5-pro-001\nmodels/gemini-1.5-pro-002\nmodels/gemini-1.5-pro\nmodels/gemini-1.5-pro-exp-0801\nmodels/gemini-1.5-pro-exp-0827\nmodels/gemini-1.5-flash-latest\nmodels/gemini-1.5-flash-001\nmodels/gemini-1.5-flash-001-tuning\nmodels/gemini-1.5-flash\nmodels/gemini-1.5-flash-exp-0827\nmodels/gemini-1.5-flash-002\nmodels/gemini-1.5-flash-8b\nmodels/gemini-1.5-flash-8b-001\nmodels/gemini-1.5-flash-8b-latest\nmodels/gemini-1.5-flash-8b-exp-0827\nmodels/gemini-1.5-flash-8b-exp-0924\nmodels/embedding-001\nmodels/text-embedding-004\nmodels/aqa\n","output_type":"stream"}]},{"cell_type":"code","source":"for model in genai.list_models():\n  if model.name == 'models/gemini-1.5-flash':\n    print(model)\n    break","metadata":{"execution":{"iopub.status.busy":"2024-11-15T08:28:58.691364Z","iopub.execute_input":"2024-11-15T08:28:58.692342Z","iopub.status.idle":"2024-11-15T08:28:58.861399Z","shell.execute_reply.started":"2024-11-15T08:28:58.692299Z","shell.execute_reply":"2024-11-15T08:28:58.860196Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"Model(name='models/gemini-1.5-flash',\n      base_model_id='',\n      version='001',\n      display_name='Gemini 1.5 Flash',\n      description='Fast and versatile multimodal model for scaling across diverse tasks',\n      input_token_limit=1000000,\n      output_token_limit=8192,\n      supported_generation_methods=['generateContent', 'countTokens'],\n      temperature=1.0,\n      max_temperature=2.0,\n      top_p=0.95,\n      top_k=40)\n","output_type":"stream"}]},{"cell_type":"code","source":"short_model = genai.GenerativeModel(\n    'gemini-1.5-flash',\n    generation_config=genai.GenerationConfig(max_output_tokens=200))\n\nresponse = short_model.generate_content('Write a 1000 word essay on the importance of crypto.')\nprint(response.text)","metadata":{"execution":{"iopub.status.busy":"2024-11-15T08:29:49.682397Z","iopub.execute_input":"2024-11-15T08:29:49.682816Z","iopub.status.idle":"2024-11-15T08:29:51.475491Z","shell.execute_reply.started":"2024-11-15T08:29:49.682778Z","shell.execute_reply":"2024-11-15T08:29:51.474060Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"## The Enduring Importance of Cryptocurrencies: Beyond the Hype\n\nCryptocurrencies, a seemingly volatile and often misunderstood sector, are rapidly evolving beyond their initial image as a speculative investment. Their underlying technology, blockchain, and the inherent properties of decentralized digital currencies are driving significant changes across various sectors, shaping a future characterized by increased transparency, security, and financial inclusion. While the speculative bubble surrounding certain cryptocurrencies has understandably attracted criticism, the fundamental importance of the technology and its potential applications warrant a closer examination.\n\nOne of the most significant aspects of crypto is its potential to revolutionize financial systems. Traditional finance is characterized by intermediaries – banks, payment processors, and governments – who control the flow of money and often extract significant fees. This centralized structure creates bottlenecks, increases costs, and can be vulnerable to manipulation and censorship. Cryptocurrencies, on the other hand, offer a decentralized alternative. Transactions are facilitated through a distributed network, eliminating the need for intermediaries and drastically reducing transaction costs. This can be particularly impactful\n","output_type":"stream"}]},{"cell_type":"code","source":"response = short_model.generate_content('Write a short poem on the importance of crypto.')\nprint(response.text)","metadata":{"execution":{"iopub.status.busy":"2024-11-15T08:30:15.110449Z","iopub.execute_input":"2024-11-15T08:30:15.110898Z","iopub.status.idle":"2024-11-15T08:30:16.085188Z","shell.execute_reply.started":"2024-11-15T08:30:15.110854Z","shell.execute_reply":"2024-11-15T08:30:16.084024Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"A ledger bright, a coded gleam,\nBeyond the banks, a flowing stream.\nOf digital gold, a future bold,\nA story new, yet to unfold.\n\nFrom chains secure, a trust takes flight,\nDecentralized, a powerful light.\nThough risks remain, a path unfolds,\nIn crypto's heart, the future holds.\n\n","output_type":"stream"}]},{"cell_type":"code","source":"from google.api_core import retry\n\nhigh_temp_model = genai.GenerativeModel(\n    'gemini-1.5-flash',\n    generation_config=genai.GenerationConfig(temperature=2.0))\n\n\n# When running lots of queries, it's a good practice to use a retry policy so your code\n# automatically retries when hitting Resource Exhausted (quota limit) errors.\nretry_policy = {\n    \"retry\": retry.Retry(predicate=retry.if_transient_error, initial=10, multiplier=1.5, timeout=300)\n}\n\nfor _ in range(5):\n  response = high_temp_model.generate_content('Pick a random colour... (respond in a single word)',\n                                              request_options=retry_policy)\n  if response.parts:\n    print(response.text, '-' * 25)","metadata":{"execution":{"iopub.status.busy":"2024-11-15T08:32:21.743358Z","iopub.execute_input":"2024-11-15T08:32:21.744406Z","iopub.status.idle":"2024-11-15T08:32:24.015283Z","shell.execute_reply.started":"2024-11-15T08:32:21.744359Z","shell.execute_reply":"2024-11-15T08:32:24.014179Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"Marigold\n -------------------------\nPurple\n -------------------------\nMarigold\n -------------------------\nMaroon\n -------------------------\nAquamarine\n -------------------------\n","output_type":"stream"}]},{"cell_type":"code","source":"low_temp_model = genai.GenerativeModel(\n    'gemini-1.5-flash',\n    generation_config=genai.GenerationConfig(temperature=0.0))\n\nfor _ in range(5):\n  response = low_temp_model.generate_content('Pick a random colour... (respond in a single word)',\n                                             request_options=retry_policy)\n  if response.parts:\n    print(response.text, '-' * 25)","metadata":{"execution":{"iopub.status.busy":"2024-11-15T08:32:48.112994Z","iopub.execute_input":"2024-11-15T08:32:48.113456Z","iopub.status.idle":"2024-11-15T08:32:50.354946Z","shell.execute_reply.started":"2024-11-15T08:32:48.113402Z","shell.execute_reply":"2024-11-15T08:32:50.353809Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"Maroon\n -------------------------\nMaroon\n -------------------------\nMaroon\n -------------------------\nMaroon\n -------------------------\nMaroon\n -------------------------\n","output_type":"stream"}]},{"cell_type":"code","source":"model = genai.GenerativeModel(\n    'gemini-1.5-flash-001',\n    generation_config=genai.GenerationConfig(\n        # These are the default values for gemini-1.5-flash-001.\n        temperature=1.0,\n        top_k=64,\n        top_p=0.95,\n    ))\n\nstory_prompt = \"You are a creative writer. Write a short story about a man who gets rich using crypto.\"\nresponse = model.generate_content(story_prompt, request_options=retry_policy)\nprint(response.text)","metadata":{"execution":{"iopub.status.busy":"2024-11-15T08:38:03.830689Z","iopub.execute_input":"2024-11-15T08:38:03.831133Z","iopub.status.idle":"2024-11-15T08:38:07.956989Z","shell.execute_reply.started":"2024-11-15T08:38:03.831094Z","shell.execute_reply":"2024-11-15T08:38:07.955875Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"Bartholomew \"Bart\" Finkleman, a man whose most daring adventure involved choosing the wrong kind of coffee creamer, stumbled into the world of cryptocurrency like a bumbling, bewildered tourist in a bustling bazaar. \n\nIt began with a friend’s drunken rave about Bitcoin, a digital gold, a future currency, a revolutionary financial system. Bart, a man who found the concept of paying bills with a physical check too daring, listened with glazed eyes, occasionally nodding and muttering, “That’s… interesting.” \n\nHe bought a few hundred dollars worth of Bitcoin as a birthday gift for his nephew, a young tech enthusiast. The nephew, a whiz kid with a digital currency app named \"Crypto-Kraken,\" took one look at the paltry sum and sighed, \"Uncle Bart, you're so old school.\" \n\nBart, stung by the lack of respect, took a second look at the Crypto-Kraken app. Suddenly, the digital currency world became less a dizzying bazaar and more like a giant, glittering game of chance. He began to dabble, hesitantly at first, then with increasing fervor. He watched charts, read white papers, and joined online communities, soaking up information like a sponge.\n\nHe stumbled upon a new coin, an \"Ethereum killer,\" dubbed \"Ethereum Slayer.\" He poured all his savings, a grand total of $5,000, into the coin, feeling a surge of adrenaline that had never visited him before. \n\nEthereum Slayer, true to its name, began to climb. In a matter of months, it skyrocketed, a digital rocket powered by hype and speculation. Bart watched, his mouth agape, as his investment grew tenfold, then twentyfold. He felt a giddy euphoria, a sense of power he had never known.\n\nBart, the man who once worried about losing a dollar in the laundromat, became a crypto millionaire. He bought a flashy sports car, a beach house, and a wardrobe that included a collection of shirts with the Ethereum Slayer logo emblazoned across the chest. He even started a crypto podcast, where he shared his newfound wisdom with the world, often punctuated with self-deprecating jokes about his past life as a \"coffee creamer connoisseur.\"\n\nHe felt a sense of validation, a feeling of having finally achieved something truly extraordinary. \n\nBut as quickly as it had risen, Ethereum Slayer began to plummet. The market crashed, a wave of disillusionment washing over the digital world. Bart, like many others, was swept away. He lost everything, his fortune evaporating like mist in the morning sun.\n\nStanding on the beach, gazing at the vast, indifferent ocean, Bart felt a pang of regret. He realized that the pursuit of wealth had been a mirage, a shimmering oasis in a desert of his own making.  He had become obsessed with the numbers, the charts, the endless cycle of buying and selling. He had forgotten about the simple pleasures, the quiet moments, the joy of a cup of coffee, even if it was the wrong kind of creamer.\n\nBart, humbled, decided to return to his old life, a life where the greatest challenge involved choosing the right kind of coffee creamer. But this time, he had a new perspective. He learned that true wealth wasn’t measured in coins, but in the quiet moments of contentment, the warmth of connection, and the simple joys of life. And he knew, with a newfound clarity, that sometimes, the best kind of investment was in oneself. \n\n","output_type":"stream"}]},{"cell_type":"code","source":"model = genai.GenerativeModel(\n    'gemini-1.5-flash-001',\n    generation_config=genai.GenerationConfig(\n        temperature=0.1,\n        top_p=1,\n        max_output_tokens=5,\n    ))\n\nzero_shot_prompt = \"\"\"Classify movie reviews as POSITIVE, NEUTRAL or NEGATIVE.\nReview: \"Her\" is a disturbing study revealing the direction\nhumanity is headed if AI is allowed to keep evolving,\nunchecked. I wish there were more movies like this masterpiece.\nSentiment: \"\"\"\n\nresponse = model.generate_content(zero_shot_prompt, request_options=retry_policy)\nprint(response.text)","metadata":{"execution":{"iopub.status.busy":"2024-11-15T08:39:09.365479Z","iopub.execute_input":"2024-11-15T08:39:09.365909Z","iopub.status.idle":"2024-11-15T08:39:09.773826Z","shell.execute_reply.started":"2024-11-15T08:39:09.365867Z","shell.execute_reply":"2024-11-15T08:39:09.772699Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"Sentiment: **POSITIVE**\n","output_type":"stream"}]},{"cell_type":"code","source":"import enum\n\nclass Sentiment(enum.Enum):\n    POSITIVE = \"positive\"\n    NEUTRAL = \"neutral\"\n    NEGATIVE = \"negative\"\n\n\nmodel = genai.GenerativeModel(\n    'gemini-1.5-flash-001',\n    generation_config=genai.GenerationConfig(\n        response_mime_type=\"text/x.enum\",\n        response_schema=Sentiment\n    ))\n\nresponse = model.generate_content(zero_shot_prompt, request_options=retry_policy)\nprint(response.text)","metadata":{"execution":{"iopub.status.busy":"2024-11-15T08:40:08.467418Z","iopub.execute_input":"2024-11-15T08:40:08.468285Z","iopub.status.idle":"2024-11-15T08:40:09.127117Z","shell.execute_reply.started":"2024-11-15T08:40:08.468238Z","shell.execute_reply":"2024-11-15T08:40:09.125870Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"positive\n","output_type":"stream"}]},{"cell_type":"code","source":"model = genai.GenerativeModel(\n    'gemini-1.5-flash-latest',\n    generation_config=genai.GenerationConfig(\n        temperature=0.1,\n        top_p=1,\n        max_output_tokens=250,\n    ))\n\nfew_shot_prompt = \"\"\"Parse a customer's pizza order into valid JSON:\n\nEXAMPLE:\nI want a small pizza with cheese, tomato sauce, and pepperoni.\nJSON Response:\n```\n{\n\"size\": \"small\",\n\"type\": \"normal\",\n\"ingredients\": [\"cheese\", \"tomato sauce\", \"peperoni\"]\n}\n```\n\nEXAMPLE:\nCan I get a large pizza with tomato sauce, basil and mozzarella\nJSON Response:\n```\n{\n\"size\": \"large\",\n\"type\": \"normal\",\n\"ingredients\": [\"tomato sauce\", \"basil\", \"mozzarella\"]\n}\n\nORDER:\n\"\"\"\n\ncustomer_order = \"Give me a large with cheese & pineapple\"\n\n\nresponse = model.generate_content([few_shot_prompt, customer_order], request_options=retry_policy)\nprint(response.text)","metadata":{"execution":{"iopub.status.busy":"2024-11-15T08:42:31.870771Z","iopub.execute_input":"2024-11-15T08:42:31.871821Z","iopub.status.idle":"2024-11-15T08:42:32.553507Z","shell.execute_reply.started":"2024-11-15T08:42:31.871752Z","shell.execute_reply":"2024-11-15T08:42:32.552450Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"```json\n{\n  \"size\": \"large\",\n  \"type\": \"normal\",\n  \"ingredients\": [\"cheese\", \"pineapple\"]\n}\n```\n\n","output_type":"stream"}]},{"cell_type":"code","source":"import typing_extensions as typing\n\nclass PizzaOrder(typing.TypedDict):\n    size: str\n    ingredients: list[str]\n    type: str\n\n\nmodel = genai.GenerativeModel(\n    'gemini-1.5-flash-latest',\n    generation_config=genai.GenerationConfig(\n        temperature=0.1,\n        response_mime_type=\"application/json\",\n        response_schema=PizzaOrder,\n    ))\n\nresponse = model.generate_content(\"Can I have a large dessert pizza with apple and chocolate\")\nprint(response.text)","metadata":{"execution":{"iopub.status.busy":"2024-11-15T08:42:53.366149Z","iopub.execute_input":"2024-11-15T08:42:53.366883Z","iopub.status.idle":"2024-11-15T08:42:54.087671Z","shell.execute_reply.started":"2024-11-15T08:42:53.366839Z","shell.execute_reply":"2024-11-15T08:42:54.086603Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"{\"ingredients\": [\"apple\", \"chocolate\"], \"size\": \"large\", \"type\": \"dessert pizza\"}\n\n","output_type":"stream"}]},{"cell_type":"code","source":"prompt = \"\"\"When I was 4 years old, my partner was 3 times my age. Now, I\nam 20 years old. How old is my partner? Return the answer directly.\"\"\"\n\nmodel = genai.GenerativeModel('gemini-1.5-flash-latest')\nresponse = model.generate_content(prompt, request_options=retry_policy)\n\nprint(response.text)","metadata":{"execution":{"iopub.status.busy":"2024-11-15T08:43:27.935646Z","iopub.execute_input":"2024-11-15T08:43:27.936065Z","iopub.status.idle":"2024-11-15T08:43:28.406622Z","shell.execute_reply.started":"2024-11-15T08:43:27.936024Z","shell.execute_reply":"2024-11-15T08:43:28.405525Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"44\n\n","output_type":"stream"}]},{"cell_type":"code","source":"prompt = \"\"\"When I was 4 years old, my partner was 3 times my age. Now,\nI am 20 years old. How old is my partner? Let's think step by step.\"\"\"\n\nresponse = model.generate_content(prompt, request_options=retry_policy)\nprint(response.text)","metadata":{"execution":{"iopub.status.busy":"2024-11-15T08:43:43.522750Z","iopub.execute_input":"2024-11-15T08:43:43.523178Z","iopub.status.idle":"2024-11-15T08:43:44.930181Z","shell.execute_reply.started":"2024-11-15T08:43:43.523135Z","shell.execute_reply":"2024-11-15T08:43:44.929091Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stdout","text":"Step 1: Find the partner's age when you were 4.\n\n* When you were 4, your partner was 3 times your age, so they were 3 * 4 = 12 years old.\n\nStep 2: Find the age difference between you and your partner.\n\n* The age difference is 12 - 4 = 8 years.\n\nStep 3: Determine your partner's current age.\n\n* You are now 20 years old.\n* Your partner is 8 years older than you.\n* Therefore, your partner is 20 + 8 = 28 years old.\n\nSo the answer is $\\boxed{28}$\n\n","output_type":"stream"}]},{"cell_type":"code","source":"model_instructions = \"\"\"\nSolve a question answering task with interleaving Thought, Action, Observation steps. Thought can reason about the current situation,\nObservation is understanding relevant information from an Action's output and Action can be one of three types:\n (1) <search>entity</search>, which searches the exact entity on Wikipedia and returns the first paragraph if it exists. If not, it\n     will return some similar entities to search and you can try to search the information from those topics.\n (2) <lookup>keyword</lookup>, which returns the next sentence containing keyword in the current context. This only does exact matches,\n     so keep your searches short.\n (3) <finish>answer</finish>, which returns the answer and finishes the task.\n\"\"\"\n\nexample1 = \"\"\"Question\nMusician and satirist Allie Goertz wrote a song about the \"The Simpsons\" character Milhouse, who Matt Groening named after who?\n\nThought 1\nThe question simplifies to \"The Simpsons\" character Milhouse is named after who. I only need to search Milhouse and find who it is named after.\n\nAction 1\n<search>Milhouse</search>\n\nObservation 1\nMilhouse Mussolini Van Houten is a recurring character in the Fox animated television series The Simpsons voiced by Pamela Hayden and created by Matt Groening.\n\nThought 2\nThe paragraph does not tell who Milhouse is named after, maybe I can look up \"named after\".\n\nAction 2\n<lookup>named after</lookup>\n\nObservation 2\nMilhouse was named after U.S. president Richard Nixon, whose middle name was Milhous.\n\nThought 3\nMilhouse was named after U.S. president Richard Nixon, so the answer is Richard Nixon.\n\nAction 3\n<finish>Richard Nixon</finish>\n\"\"\"\n\nexample2 = \"\"\"Question\nWhat is the elevation range for the area that the eastern sector of the Colorado orogeny extends into?\n\nThought 1\nI need to search Colorado orogeny, find the area that the eastern sector of the Colorado orogeny extends into, then find the elevation range of the area.\n\nAction 1\n<search>Colorado orogeny</search>\n\nObservation 1\nThe Colorado orogeny was an episode of mountain building (an orogeny) in Colorado and surrounding areas.\n\nThought 2\nIt does not mention the eastern sector. So I need to look up eastern sector.\n\nAction 2\n<lookup>eastern sector</lookup>\n\nObservation 2\nThe eastern sector extends into the High Plains and is called the Central Plains orogeny.\n\nThought 3\nThe eastern sector of Colorado orogeny extends into the High Plains. So I need to search High Plains and find its elevation range.\n\nAction 3\n<search>High Plains</search>\n\nObservation 3\nHigh Plains refers to one of two distinct land regions\n\nThought 4\nI need to instead search High Plains (United States).\n\nAction 4\n<search>High Plains (United States)</search>\n\nObservation 4\nThe High Plains are a subregion of the Great Plains. From east to west, the High Plains rise in elevation from around 1,800 to 7,000 ft (550 to 2,130m).\n\nThought 5\nHigh Plains rise in elevation from around 1,800 to 7,000 ft, so the answer is 1,800 to 7,000 ft.\n\nAction 5\n<finish>1,800 to 7,000 ft</finish>\n\"\"\"","metadata":{"execution":{"iopub.status.busy":"2024-11-15T08:44:48.567005Z","iopub.execute_input":"2024-11-15T08:44:48.567496Z","iopub.status.idle":"2024-11-15T08:44:48.576591Z","shell.execute_reply.started":"2024-11-15T08:44:48.567451Z","shell.execute_reply":"2024-11-15T08:44:48.575302Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"question = \"\"\"Question\nWho was the youngest author listed on the transformers NLP paper?\n\"\"\"\n\nmodel = genai.GenerativeModel('gemini-1.5-flash-latest')\nreact_chat = model.start_chat()\n\n# You will perform the Action, so generate up to, but not including, the Observation.\nconfig = genai.GenerationConfig(stop_sequences=[\"\\nObservation\"])\n\nresp = react_chat.send_message(\n    [model_instructions, example1, example2, question],\n    generation_config=config,\n    request_options=retry_policy)\nprint(resp.text)","metadata":{"execution":{"iopub.status.busy":"2024-11-15T08:45:03.091906Z","iopub.execute_input":"2024-11-15T08:45:03.092764Z","iopub.status.idle":"2024-11-15T08:45:06.012223Z","shell.execute_reply.started":"2024-11-15T08:45:03.092717Z","shell.execute_reply":"2024-11-15T08:45:06.011126Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stdout","text":"Thought 1\nI need to find the Transformers NLP paper and identify the authors.  Then I need to determine who was the youngest.  This will require searching for the paper and then likely some external information to find the authors' ages.\n\nAction 1\n<search>Transformers NLP paper</search>\n\n","output_type":"stream"}]},{"cell_type":"code","source":"observation = \"\"\"Observation 1\n[1706.03762] Attention Is All You Need\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin\nWe propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely.\n\"\"\"\nresp = react_chat.send_message(observation, generation_config=config, request_options=retry_policy)\nprint(resp.text)","metadata":{"execution":{"iopub.status.busy":"2024-11-15T08:45:21.358995Z","iopub.execute_input":"2024-11-15T08:45:21.360057Z","iopub.status.idle":"2024-11-15T08:45:24.052399Z","shell.execute_reply.started":"2024-11-15T08:45:21.360006Z","shell.execute_reply":"2024-11-15T08:45:24.050838Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stdout","text":"Thought 2\nThe observation provides the authors of the paper \"Attention is All You Need.\"  I need to find their ages to determine the youngest.  This will require additional actions.  I cannot directly determine age from this text.\n\nAction 2\n<search>Ashish Vaswani age>\n\n\n","output_type":"stream"}]},{"cell_type":"code","source":"model = genai.GenerativeModel(\n    'gemini-1.5-flash-latest',\n    generation_config=genai.GenerationConfig(\n        temperature=1,\n        top_p=1,\n        max_output_tokens=1024,\n    ))\n\n# Gemini 1.5 models are very chatty, so it helps to specify they stick to the code.\ncode_prompt = \"\"\"\nWrite a Python function to calculate the factorial of a number. No explanation, provide only the code.\n\"\"\"\n\nresponse = model.generate_content(code_prompt, request_options=retry_policy)\nMarkdown(response.text)","metadata":{"execution":{"iopub.status.busy":"2024-11-15T08:45:43.989149Z","iopub.execute_input":"2024-11-15T08:45:43.989590Z","iopub.status.idle":"2024-11-15T08:45:44.677655Z","shell.execute_reply.started":"2024-11-15T08:45:43.989548Z","shell.execute_reply":"2024-11-15T08:45:44.676583Z"},"trusted":true},"execution_count":26,"outputs":[{"execution_count":26,"output_type":"execute_result","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"```python\ndef factorial(n):\n  if n == 0:\n    return 1\n  else:\n    return n * factorial(n-1)\n```\n"},"metadata":{}}]},{"cell_type":"code","source":"model = genai.GenerativeModel(\n    'gemini-1.5-flash-latest',\n    tools='code_execution',)\n\ncode_exec_prompt = \"\"\"\nCalculate the sum of the first 14 prime numbers. Only consider the odd primes, and make sure you count them all.\n\"\"\"\n\nresponse = model.generate_content(code_exec_prompt, request_options=retry_policy)\nMarkdown(response.text)","metadata":{"execution":{"iopub.status.busy":"2024-11-15T08:45:59.718407Z","iopub.execute_input":"2024-11-15T08:45:59.718874Z","iopub.status.idle":"2024-11-15T08:46:02.415694Z","shell.execute_reply.started":"2024-11-15T08:45:59.718832Z","shell.execute_reply":"2024-11-15T08:46:02.414432Z"},"trusted":true},"execution_count":27,"outputs":[{"execution_count":27,"output_type":"execute_result","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"To calculate the sum of the first 14 odd prime numbers, I need to first identify those primes.  The first few odd prime numbers are 3, 5, 7, 11, 13, and so on.  I will use Python to generate these primes and then sum them.\n\n\n``` python\ndef is_prime(n):\n    \"\"\"Checks if a number is prime.\"\"\"\n    if n <= 1:\n        return False\n    for i in range(2, int(n**0.5) + 1):\n        if n % i == 0:\n            return False\n    return True\n\ncount = 0\nsum_of_primes = 0\nnum = 3\nwhile count < 14:\n    if is_prime(num):\n        sum_of_primes += num\n        count += 1\n    num += 2\n\nprint(f\"The sum of the first 14 odd prime numbers is: {sum_of_primes}\")\n\n\n```\n```\nThe sum of the first 14 odd prime numbers is: 326\n\n```\nTherefore, the sum of the first 14 odd prime numbers is 326.\n"},"metadata":{}}]},{"cell_type":"code","source":"for part in response.candidates[0].content.parts:\n  print(part)\n  print(\"-----\")","metadata":{"execution":{"iopub.status.busy":"2024-11-15T08:46:18.322374Z","iopub.execute_input":"2024-11-15T08:46:18.322813Z","iopub.status.idle":"2024-11-15T08:46:18.330258Z","shell.execute_reply.started":"2024-11-15T08:46:18.322768Z","shell.execute_reply":"2024-11-15T08:46:18.328901Z"},"trusted":true},"execution_count":28,"outputs":[{"name":"stdout","text":"text: \"To calculate the sum of the first 14 odd prime numbers, I need to first identify those primes.  The first few odd prime numbers are 3, 5, 7, 11, 13, and so on.  I will use Python to generate these primes and then sum them.\\n\\n\"\n\n-----\nexecutable_code {\n  language: PYTHON\n  code: \"\\ndef is_prime(n):\\n    \\\"\\\"\\\"Checks if a number is prime.\\\"\\\"\\\"\\n    if n <= 1:\\n        return False\\n    for i in range(2, int(n**0.5) + 1):\\n        if n % i == 0:\\n            return False\\n    return True\\n\\ncount = 0\\nsum_of_primes = 0\\nnum = 3\\nwhile count < 14:\\n    if is_prime(num):\\n        sum_of_primes += num\\n        count += 1\\n    num += 2\\n\\nprint(f\\\"The sum of the first 14 odd prime numbers is: {sum_of_primes}\\\")\\n\\n\"\n}\n\n-----\ncode_execution_result {\n  outcome: OUTCOME_OK\n  output: \"The sum of the first 14 odd prime numbers is: 326\\n\"\n}\n\n-----\ntext: \"Therefore, the sum of the first 14 odd prime numbers is 326.\\n\"\n\n-----\n","output_type":"stream"}]},{"cell_type":"code","source":"file_contents = !curl https://raw.githubusercontent.com/magicmonty/bash-git-prompt/refs/heads/master/gitprompt.sh\n\nexplain_prompt = f\"\"\"\nPlease explain what this file does at a very high level. What is it, and why would I use it?\n\n```\n{file_contents}\n```\n\"\"\"\n\nmodel = genai.GenerativeModel('gemini-1.5-flash-latest')\n\nresponse = model.generate_content(explain_prompt, request_options=retry_policy)\nMarkdown(response.text)","metadata":{"execution":{"iopub.status.busy":"2024-11-15T08:46:34.552627Z","iopub.execute_input":"2024-11-15T08:46:34.553072Z","iopub.status.idle":"2024-11-15T08:46:38.102916Z","shell.execute_reply.started":"2024-11-15T08:46:34.553027Z","shell.execute_reply":"2024-11-15T08:46:38.101758Z"},"trusted":true},"execution_count":29,"outputs":[{"execution_count":29,"output_type":"execute_result","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"This file is a bash script that enhances your shell prompt to display information about your Git repository.  In essence, it's a **Git prompt customization script**.\n\nYou would use it to:\n\n* **Improve your shell prompt:** Instead of a plain prompt, you'll see information like your current branch, status (ahead/behind, staged changes, etc.), and potentially even the remote repository URL. This provides a quick at-a-glance view of your Git project's state without needing to run `git status` manually.\n\n* **Customize the prompt appearance:** The script allows for extensive customization of colors, symbols, and the overall layout of the prompt information, using themes (including a facility for creating custom themes).\n\n* **Integrate with various shells:** It aims for compatibility with both Bash and Zsh.\n\nThe script does this by defining a series of functions that:\n\n1. **Detect the Git repository:** Checks if the current directory is within a Git repository.\n2. **Fetch Git status:** Executes `git status` (or a customizable command) to retrieve the repository's state.\n3. **Parse the status:** Extracts relevant information (branch, changes, etc.).\n4. **Format the output:**  Applies colors and symbols based on the chosen theme and configuration.\n5. **Integrate into the prompt:** Modifies the `PS1` variable (which controls your prompt) to include the formatted Git information.\n\nIt's highly configurable through environment variables and a configuration file (`.bash-git-rc` within the Git repository), allowing fine-grained control over what information is displayed and how it looks.  The extensive use of functions makes it modular and maintainable.  The download progress at the beginning of the file suggests it might have been downloaded from a source like GitHub.\n"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}